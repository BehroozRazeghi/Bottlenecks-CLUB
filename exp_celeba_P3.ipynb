{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Dropout,BatchNormalization,Lambda,Activation,Input,Flatten,Conv2DTranspose,concatenate,Reshape,LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set some options and read file names and attribute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#options\n",
    "IMG_DIM = 64\n",
    "DIM_S = 2\n",
    "supervised = False #False:Unsupervised\n",
    "prior_dist = 0 # 0: Isotropic 1: guassian mixture\n",
    "sensitive = 'emotion' #'gender' or 'emotion'\n",
    "\n",
    "\n",
    "\n",
    "# automatically set variables based on selected options\n",
    "DIM_U = 2 if supervised else [IMG_DIM, IMG_DIM, 64]\n",
    "utility = 'emotion' if sensitive == 'gender' else 'gender'\n",
    "prior_type = 'Isotropic' if prior_dist == 0 else 'Mixture'\n",
    "exp_info = f\"s_{sensitive}_u_{utility}_{prior_type}_supervised\" if supervised else f\"s_{sensitive}_{prior_type}_unsupervised\"\n",
    "exp_info += \"_P3\"\n",
    "\n",
    "#Folder and image files\n",
    "generalFolder = './CelebA/'\n",
    "folder = generalFolder + 'img_align_celeba/'\n",
    "allFiles = [folder + \"/\" + f for f in sorted(os.listdir(folder))]#Folder and image files\n",
    "n = len(allFiles)\n",
    "print(\"Number of data : \", n)\n",
    "\n",
    "df = pd.read_csv(generalFolder + 'list_attr_celeba.csv', nrows=n)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading U and S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = df['Male'].to_numpy() if(sensitive == 'gender') else df['Smiling'].to_numpy()\n",
    "S[S == -1] = 0\n",
    "\n",
    "if supervised:\n",
    "    U = df['Smiling'].to_numpy() if(sensitive == 'gender') else df['Male'].to_numpy()\n",
    "    U[U == -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading partion information for split data into test, train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_part = pd.read_csv(generalFolder + 'list_eval_partition.txt', header=None, sep=\" \").to_numpy()[:, 1].astype(int)\n",
    "print(eval_part.shape)\n",
    "n_train = sum(eval_part == 0)\n",
    "n_valid = sum(eval_part == 1)\n",
    "n_test  = sum(eval_part == 2)\n",
    "print(n_train, n_valid, n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot U and S in order to ready them for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.zeros((n_train, IMG_DIM, IMG_DIM, 3))\n",
    "x_valid = np.zeros((n_valid, IMG_DIM, IMG_DIM, 3))\n",
    "x_test = np.zeros((n_test, IMG_DIM, IMG_DIM, 3))\n",
    "\n",
    "print(\"Reading train data:\")\n",
    "for i in range(0,n_train):\n",
    "  if (i+1) % 20000 == 0:\n",
    "    print(f\"{i+1} / {n_train}\")\n",
    "  img = cv2.imread(allFiles[i])\n",
    "  img = cv2.resize(img,(IMG_DIM,IMG_DIM),interpolation = cv2.INTER_AREA)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  x_train[i, :, :, :] = img / 255.0\n",
    "\n",
    "print(\"Reading validation data:\")\n",
    "for i in range(n_train,(n_train+n_valid)):\n",
    "  if (i-n_train+1) % 5000 == 0:\n",
    "    print(f\"{i-n_train+1} / {n_valid}\")\n",
    "  img = cv2.imread(allFiles[i])\n",
    "  img = cv2.resize(img,(IMG_DIM,IMG_DIM),interpolation = cv2.INTER_AREA)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  x_valid[i-n_train, :, :, :] = img / 255.0\n",
    "    \n",
    "print(\"Reading test data:\")\n",
    "for i in range((n_train+n_valid),202599):\n",
    "  if (i-(n_train+n_valid)+1) % 5000 == 0:\n",
    "    print(f\"{i-(n_train+n_valid)+1} / {n_test}\")\n",
    "  img = cv2.imread(allFiles[i])\n",
    "  img = cv2.resize(img,(IMG_DIM,IMG_DIM),interpolation = cv2.INTER_AREA)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  x_test[i-(n_train+n_valid), :, :, :] = img / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, num_labels):\n",
    "    num_labels_data = labels.shape[0]\n",
    "    one_hot_encoding = np.zeros((num_labels_data,num_labels))\n",
    "    one_hot_encoding[np.arange(num_labels_data),labels] = 1\n",
    "    one_hot_encoding = np.reshape(one_hot_encoding, [-1, num_labels])\n",
    "    return one_hot_encoding\n",
    "\n",
    "s_train = one_hot(S[:n_train], DIM_S).astype(np.float32)\n",
    "s_valid = one_hot(S[n_train:(n_train+n_valid)], DIM_S).astype(np.float32)\n",
    "s_test = one_hot(S[(n_train+n_valid):], DIM_S).astype(np.float32)\n",
    "\n",
    "if not supervised:\n",
    "    u_train = x_train\n",
    "    u_valid = x_valid\n",
    "    u_test = x_test\n",
    "else:\n",
    "    u_train = one_hot(U[:n_train], DIM_U).astype(np.float32)\n",
    "    u_valid = one_hot(U[n_train:(n_train+n_valid)], DIM_U).astype(np.float32)\n",
    "    u_test = one_hot(U[(n_train+n_valid):], DIM_U).astype(np.float32)\n",
    "\n",
    "print(s_train.shape, s_valid.shape, s_test.shape)    \n",
    "print(u_train.shape, u_valid.shape, u_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(DIM_Z, input_x):\n",
    "    stride = 2\n",
    "#     input_x = Input( shape = [IMG_DIM,IMG_DIM,3], name=\"x\" )\n",
    "\n",
    "    #first hidden layer\n",
    "    x = Conv2D(16, 3, strides=stride, padding=\"same\", name=\"enc_h1\")(input_x)\n",
    "    x = BatchNormalization(name=\"enc_h1_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"enc_h1_activation\")(x)\n",
    "    #second hidden layer\n",
    "    x = Conv2D(32, 3, strides=stride, padding=\"same\", name=\"enc_h2\")(x)\n",
    "    x = BatchNormalization(name=\"enc_h2_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"enc_h2_activation\")(x)\n",
    "    #third hidden layer\n",
    "    x = Conv2D(64, 3, strides=stride, padding=\"same\", name=\"enc_h3\")(x)\n",
    "    x = BatchNormalization(name=\"enc_h3_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"enc_h3_activation\")(x)\n",
    "    #forth hidden layer\n",
    "    x = Conv2D(128, 3, strides=stride, padding=\"same\", name=\"enc_h4\")(x)\n",
    "    x = BatchNormalization(name=\"enc_h4_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"enc_h4_activation\")(x)\n",
    "    #fifth hidden layer\n",
    "    x = Conv2D(256, 3, strides=stride, padding=\"same\", name=\"enc_h5\")(x)\n",
    "    x = BatchNormalization(name=\"enc_h5_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"enc_h5_activation\")(x)\n",
    "    \n",
    "    shape = K.int_shape(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(DIM_Z*4, name=\"enc_dense_1\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)\n",
    "\n",
    "    z_mean = Dense(DIM_Z, name=\"z_mean\")(x)\n",
    "    z_log_sigma_sq = Dense(DIM_Z, name=\"z_sigma\")(x)\n",
    "    z = Lambda(sampling, output_shape=DIM_Z, name='lambda_z')([z_mean, z_log_sigma_sq])\n",
    "    \n",
    "#     prior_loss = K.mean(-0.5 * K.sum(1 + z_log_sigma_sq - K.square(z_mean) - K.exp(z_log_sigma_sq), axis=-1))\n",
    "    \n",
    "    encoder = Model(input_x, z, name = \"Encoder\")\n",
    "#     encoder.add_loss((alpha+beta) * prior_loss)\n",
    "    \n",
    "    return (encoder, z_mean, z_log_sigma_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty_model(DIM_Z):\n",
    "    model = Sequential(name=\"Uncertainty_Decoder\")\n",
    "    model.add(Dense(DIM_Z * 4, input_dim=DIM_Z))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(DIM_S, activation='softmax', name=\"s_hat\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if supervised:\n",
    "    def get_utility_model(DIM_Z):\n",
    "        model = Sequential(name=\"Utility_Decoder_supervised\")\n",
    "        model.add(Dense(DIM_Z, input_dim=DIM_Z))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(DIM_U, activation='softmax', name=\"u_hat\"))\n",
    "        return model\n",
    "else:\n",
    "    def get_utility_model(DIM_Z):\n",
    "        stride = 2\n",
    "        model = Sequential(name=\"Utility_Decoder_unsupervised\")\n",
    "        model.add(Dense(2*2*256, input_dim=DIM_Z))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Reshape((2,2,256)))\n",
    "        \n",
    "        model.add(Conv2DTranspose(128, 3, strides=stride, padding=\"same\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2DTranspose(64, 3, strides=stride, padding=\"same\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2DTranspose(32, 3, strides=stride, padding=\"same\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2DTranspose(16, 3, strides=stride, padding=\"same\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2DTranspose(8, 3, strides=stride, padding=\"same\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2D(3, 3, strides=1, padding=\"same\", activation='sigmoid', name=\"u_hat\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z_discriminator(DIM_Z):\n",
    "    model = Sequential(name=\"Latent_Space_Discriminator\")\n",
    "    \n",
    "    model.add(Dense(512, input_dim=DIM_Z))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utility_discriminator():\n",
    "    model = Sequential(name=\"Attribute_Class_Discriminator\")\n",
    "\n",
    "    model.add(Dense(DIM_U * 8, input_dim=DIM_U))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(DIM_U * 8))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visible_space_discriminator():\n",
    "    stride = 2\n",
    "    model = Sequential(name=\"Visible_Space_Discriminator\")\n",
    "    \n",
    "    model.add(Input(shape = [IMG_DIM,IMG_DIM,3]))\n",
    "    \n",
    "    model.add(Conv2D(16, 3, strides=stride, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(32, 3, strides=stride, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(64, 3, strides=stride, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(128, 3, strides=stride, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty_discriminator():\n",
    "    model = Sequential(name=\"Uncertainty_Discriminator\")\n",
    "\n",
    "    model.add(Dense(DIM_S * 8, input_dim=DIM_S))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(DIM_S * 8))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior_generator(DIM_Z, noise_dim=100):\n",
    "    model = Sequential(name=\"Prior_Distribution_Generator\")\n",
    "    \n",
    "    model.add(Dense(noise_dim*2, input_dim=noise_dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(noise_dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(DIM_Z))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Train a model to evaluate output of the main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_model():\n",
    "    stride = 2\n",
    "    #declare inputs to the encoder, which is just x\n",
    "    input_x = Input( shape = [IMG_DIM,IMG_DIM,3], name=\"x\" )\n",
    "\n",
    "    #first hidden layer\n",
    "    x = Conv2D(32, 3, strides=stride, padding=\"same\", name=\"h1\")(input_x)\n",
    "    x = BatchNormalization(name=\"h1_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"h1_activation\")(x)\n",
    "\n",
    "    #second hidden layer\n",
    "    x = Conv2D(64, 3, strides=stride, padding=\"same\", name=\"h2\")(x)\n",
    "    x = BatchNormalization(name=\"h2_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"h2_activation\")(x)\n",
    "\n",
    "    #third hidden layer\n",
    "    x = Conv2D(128, 3, strides=stride, padding=\"same\", name=\"h3\")(x)\n",
    "    x = BatchNormalization(name=\"h3_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"h3_activation\")(x)\n",
    "\n",
    "    #forth hidden layer\n",
    "    x = Conv2D(256, 3, strides=stride, padding=\"same\", name=\"h4\")(x)\n",
    "    x = BatchNormalization(name=\"h4_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"h4_activation\")(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(1024, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = BatchNormalization(name=\"dense1_normalized\")(x)\n",
    "\n",
    "    s_hat = Dense(DIM_S, activation=\"softmax\", name='s_hat')(x)\n",
    "    \n",
    "    return Model(input_x, s_hat, name=\"CNN_Model_test_sensitivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not supervised:\n",
    "    eval_model = get_eval_model()\n",
    "    eval_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss='binary_crossentropy')\n",
    "\n",
    "    force_train = False\n",
    "\n",
    "    if not os.path.exists(f\"saved_models/celeba_eval_model_{exp_info}.h5\") or force_train:\n",
    "        print(\"Training sensitive attribute evaluator\")\n",
    "        history = eval_model.fit(x_train, s_train, validation_data=(x_valid, s_valid), batch_size=1024, epochs=50, shuffle=True, verbose=2)\n",
    "        eval_model.save_weights(f\"saved_models/celeba_eval_model_{exp_info}.h5\")\n",
    "    else:\n",
    "        print(\"Loading sensitive attribute evaluator from file\")\n",
    "        eval_model.load_weights(f\"saved_models/celeba_eval_model_{exp_info}.h5\")\n",
    "\n",
    "    ## Evaluating evaluator!\n",
    "    s_hat_test = eval_model.predict(x_test)\n",
    "    print(sum(np.argmax(s_hat_test,axis=-1)==np.argmax(s_test,axis=-1)) / s_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all togther - define CLUB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_gen_enabled = False #if False, the algorithm sample prior from Normal Distribution\n",
    "## utility and reconstruction losses\n",
    "if supervised:\n",
    "    def loss_u(gamma):\n",
    "        def loss(u_true, u_pred):\n",
    "            return gamma * K.mean(K.sum(K.square(u_true-u_pred), axis=-1))\n",
    "        return loss\n",
    "else:\n",
    "    def loss_u(gamma):\n",
    "        def loss(u_true, u_pred):\n",
    "            return gamma * K.mean(K.sum(K.square(u_true-u_pred), axis=(1,2,3)))\n",
    "        return loss\n",
    "\n",
    "def loss_s(lam):\n",
    "    def loss(s_true, s_pred):\n",
    "        return lam * -K.mean(K.sum(K.square(s_true-s_pred), axis=-1))\n",
    "    return loss\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "# Weighted cross-entropy loss\n",
    "def loss_wce(coef):\n",
    "    def loss(y, y_pred):\n",
    "         return coef * bce(y, y_pred)\n",
    "    return loss\n",
    "\n",
    "def get_full_model(DIM_Z, lam, gamma, learning_rate=0.0001, dim_noise = 100):\n",
    "    ########## Inputs\n",
    "    input_x = Input( shape=[IMG_DIM,IMG_DIM,3], name=\"x\" )\n",
    "    input_z = Input( shape = (DIM_Z,), name=\"z\" )\n",
    "    input_noise = Input( shape = (dim_noise,), name=\"Noise\")\n",
    "\n",
    "    ########## Define AE: Encoder, Utility Decoder and Uncertainty Decoder\n",
    "    encoder,z_mean,z_log_sigma_sq = get_encoder(DIM_Z, input_x)\n",
    "    uncertainty_decoder = get_uncertainty_model(DIM_Z)\n",
    "    utility_decoder = get_utility_model(DIM_Z)\n",
    "\n",
    "    z = encoder(input_x)\n",
    "    s_hat = uncertainty_decoder(z)\n",
    "    u_hat = utility_decoder(z)\n",
    "\n",
    "    autoencoder = Model(input_x, [s_hat, u_hat], name=\"CLUB_Autoencoder\")\n",
    "    prior_loss = K.mean(-0.5 * K.sum(1 + z_log_sigma_sq - K.square(z_mean) - K.exp(z_log_sigma_sq), axis=-1))\n",
    "    prior_loss = tf.identity(prior_loss, name=\"kl_loss\")\n",
    "    autoencoder.add_loss(prior_loss)\n",
    "    autoencoder.compile(loss=[loss_s(lam), loss_u(gamma)], optimizer=tf.keras.optimizers.Adam(lr=learning_rate*5))\n",
    "\n",
    "    ########## Define Latent Space Discriminator\n",
    "    z_discriminator = get_z_discriminator(DIM_Z)\n",
    "    z_discriminator.compile(loss=loss_wce(0.1), optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "    ##########\n",
    "    #z_discriminator should train separately\n",
    "    z_discriminator.trainable = False\n",
    "    prior_generator = get_prior_generator(DIM_Z, dim_noise)\n",
    "    prior_gen_zdiscriminator = Model(input_noise, z_discriminator(prior_generator(input_noise)), name=\"CLUB_generator_zdiscriminator\")\n",
    "    prior_gen_zdiscriminator.compile(loss=loss_wce(-0.1), optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "    ##########\n",
    "    encoder_zdiscriminator = Model(input_x, z_discriminator(encoder(input_x)), name=\"CLUB_encoder_zdiscriminator\")\n",
    "    encoder_zdiscriminator.compile(loss=loss_wce(-0.1), optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "    ##########\n",
    "    u_dircriminator = get_utility_discriminator() if supervised else get_visible_space_discriminator()\n",
    "    u_dircriminator.compile(loss=loss_wce(0.1*gamma), optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "    ##########\n",
    "    #u_dircriminator should train separately\n",
    "    u_dircriminator.trainable = False\n",
    "\n",
    "    if z_gen_enabled:    \n",
    "        decoder_udiscriminator = Model(input_noise, u_dircriminator(utility_decoder(prior_generator(input_noise))), name=\"CLUB_decoder_udiscriminator\")\n",
    "    else:\n",
    "        decoder_udiscriminator = Model(input_z, u_dircriminator(utility_decoder(input_z)), name=\"CLUB_decoder_udiscriminator\")\n",
    "\n",
    "    decoder_udiscriminator.compile(loss=loss_wce(-0.1*gamma), optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "    ##########\n",
    "    s_dircriminator = get_uncertainty_discriminator()\n",
    "    s_dircriminator.compile(loss=loss_wce(-0.1*lam), optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "    ##########\n",
    "    #s_dircriminator should train separately\n",
    "    s_dircriminator.trainable = False\n",
    "    if z_gen_enabled:\n",
    "        decoder_sdiscriminator = Model(input_noise, s_dircriminator(uncertainty_decoder(prior_generator(input_noise))), name=\"CLUB_decoder_sdiscriminator\")\n",
    "    else:\n",
    "        decoder_sdiscriminator = Model(input_z, s_dircriminator(uncertainty_decoder(input_z)), name=\"CLUB_decoder_sdiscriminator\")\n",
    "\n",
    "    decoder_sdiscriminator.compile(loss=loss_wce(0.1 * lam), optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\n",
    "                                   \n",
    "    return encoder, uncertainty_decoder, utility_decoder, autoencoder, z_discriminator, prior_generator, prior_gen_zdiscriminator, encoder_zdiscriminator, u_dircriminator, decoder_udiscriminator, s_dircriminator, decoder_sdiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "# encoder, uncertainty_decoder, utility_decoder, autoencoder, z_discriminator, prior_generator, prior_gen_zdiscriminator, encoder_zdiscriminator, u_dircriminator, decoder_udiscriminator, s_dircriminator, decoder_sdiscriminator= get_full_model(32,0.1,0.1)\n",
    "# tf.keras.utils.plot_model(decoder_udiscriminator, show_shapes=True)\n",
    "# tf.keras.utils.plot_model(autoencoder,show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train_ae(dim_z, lam, gamma, force_train=False, max_itr=100, batch_size=1024, verbose=2):\n",
    "    info_str = f\"d_{dim_z}_lambda_{lam}_gamma_{gamma}_{exp_info}\"\n",
    "    if not os.path.exists(f\"saved_models/celeba_pretrain_{info_str}.h5\") or force_train:\n",
    "        print(f\"Pre-Training with {info_str}\")\n",
    "        autoencoder.fit(x_train, [s_train, u_train], validation_data=(x_valid, [s_valid, u_valid]), batch_size=batch_size, epochs=max_itr, shuffle=True, verbose=verbose)\n",
    "        autoencoder.save_weights(f\"saved_models/celeba_pretrain_{info_str}.h5\")\n",
    "    else:\n",
    "        print(f\"Loading model from file with {info_str}\")\n",
    "        autoencoder.load_weights(f\"saved_models/celeba_pretrain_{info_str}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sampler import gaussian, gaussian_mixture\n",
    "\n",
    "def sample_prior(latent_dim, batch_size):\n",
    "    if prior_dist == 0:  \n",
    "        return gaussian(batch_size, latent_dim)\n",
    "    elif prior_dist == 1:\n",
    "        return gaussian_mixture(batch_size, latent_dim, num_labels=min(10, DIM_U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(dim_z, max_itr=2000, batch_size=1024, z_disc_enabled=True,u_disc_enabled=True,s_disc_enabled=True,verbose=1,dim_noise=100):\n",
    "    ones = np.ones((batch_size, 1))\n",
    "    zeros = np.zeros((batch_size, 1))\n",
    "    start_time = time.time()\n",
    "    for epoch in range(max_itr):\n",
    "        start_time_epoch = time.time()\n",
    "        # Select a random batch of images\n",
    "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        x = x_train[idx]\n",
    "        s = s_train[idx]\n",
    "        u = u_train[idx]\n",
    "        # ---------------------\n",
    "        #  1- Train the Encoder, Utility Decoder, Uncertainty Decoder\n",
    "        # ---------------------\n",
    "        ae_loss = autoencoder.train_on_batch(x, [s, u])\n",
    "\n",
    "        if z_disc_enabled:\n",
    "        # ---------------------\n",
    "        #  2- Train the Latent Space Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "            if z_gen_enabled:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[batch_size, dim_noise])\n",
    "                latent_prior = prior_generator(noise)\n",
    "            else:\n",
    "                latent_prior = sample_prior(dim_z, batch_size)\n",
    "\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            x = x_train[idx]\n",
    "            latent_enc = encoder(x)\n",
    "\n",
    "            d_loss_prior = z_discriminator.train_on_batch(latent_prior, zeros)\n",
    "            d_loss_enc = z_discriminator.train_on_batch(latent_enc, ones)\n",
    "            dz_loss = d_loss_prior + d_loss_enc\n",
    "\n",
    "            # ---------------------\n",
    "            # 3- Train the Encoder and Prior Distribution Generator Adversarially\n",
    "            # ---------------------\n",
    "            prior_loss = 0.\n",
    "            if z_gen_enabled:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[batch_size, dim_noise])\n",
    "                prior_loss = prior_gen_zdiscriminator.train_on_batch(noise, zeros)\n",
    "\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            x = x_train[idx]\n",
    "            edz_loss = encoder_zdiscriminator.train_on_batch(x, ones)\n",
    "        else:\n",
    "            dz_loss = 0.\n",
    "            prior_loss = 0.\n",
    "            edz_loss = 0.\n",
    "\n",
    "        if u_disc_enabled:\n",
    "        # ---------------------\n",
    "        #  4- Train Visible_Space/Attribute_Class Discriminator \n",
    "        # ---------------------\n",
    "\n",
    "            if z_gen_enabled:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[batch_size, dim_noise])\n",
    "                latent = prior_generator(noise)\n",
    "            else:\n",
    "                latent = sample_prior(dim_z, batch_size)\n",
    "            u_dec = utility_decoder(latent)\n",
    "\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            u = u_train[idx]\n",
    "\n",
    "            d_loss_real = u_dircriminator.train_on_batch(u, ones)\n",
    "            d_loss_fake = u_dircriminator.train_on_batch(u_dec, zeros)\n",
    "            du_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # ---------------------\n",
    "        #  5- Train the Prior Distribution Generator and Utility Decoder Adversarially\n",
    "        # ---------------------\n",
    "            if z_gen_enabled:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[batch_size, dim_noise])\n",
    "                gdu_loss = decoder_udiscriminator.train_on_batch(noise, zeros)\n",
    "            else:    \n",
    "                latent = sample_prior(dim_z, batch_size)\n",
    "                gdu_loss = decoder_udiscriminator.train_on_batch(latent, zeros)\n",
    "        else:\n",
    "            gdu_loss = 0.\n",
    "            du_loss = 0.\n",
    "\n",
    "        # ---------------------\n",
    "        #  6- Train the Uncertainty Discriminator\n",
    "        # ---------------------\n",
    "        if s_disc_enabled:\n",
    "            \n",
    "            if z_gen_enabled:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[batch_size, dim_noise])\n",
    "                latent = prior_generator(noise)\n",
    "            else:\n",
    "                latent = sample_prior(dim_z, batch_size)\n",
    "            s_dec = uncertainty_decoder(latent)\n",
    "\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            s = s_train[idx]\n",
    "\n",
    "            d_loss_1 = s_dircriminator.train_on_batch(s, ones)\n",
    "            d_loss_2 = s_dircriminator.train_on_batch(s_dec, zeros)\n",
    "            ds_loss = d_loss_1 + d_loss_2\n",
    "        # ---------------------\n",
    "        #  7- Train the Prior Distribution Generator and Uncertainty Decoder Adversarially\n",
    "        # ---------------------\n",
    "            if z_gen_enabled:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[batch_size, dim_noise])\n",
    "                gds_loss = decoder_sdiscriminator.train_on_batch(noise, zeros)\n",
    "            else:    \n",
    "                latent = sample_prior(dim_z, batch_size)\n",
    "                gds_loss = decoder_sdiscriminator.train_on_batch(latent, zeros)\n",
    "        else:\n",
    "            ds_loss = 0.\n",
    "            gds_loss = 0.\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Print stats info\n",
    "        # ---------------------\n",
    "        if verbose != 0 and epoch % 20 == 0:\n",
    "            print(f\"{epoch}, s:{ae_loss[1]:.4f}, u:{ae_loss[2]:.4f}, dz:{dz_loss:.4f}, edz:{edz_loss:.4f}, prior:{prior_loss:.4f}, du:{du_loss:.4f}, gdu:{gdu_loss:.4f}, ds:{ds_loss:.4f}, gds:{gds_loss:.4f}\")\n",
    "            print(f\"One epoch execution time: {(time.time() - start_time_epoch):.5} seconds\")\n",
    "\n",
    "    total_time = (time.time() - start_time) / 60\n",
    "    print(f\"Total Execution Time: {total_time:.4f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mine import MINE\n",
    "lam_list = [0.0001, 0.1, 10, 1000]\n",
    "gamma_list = [0.0001, 0.1, 10, 1000]\n",
    "DIM_Z = [64, 128]\n",
    "\n",
    "util_acc_tr = np.zeros((len(DIM_Z),len(lam_list),len(gamma_list)))\n",
    "util_acc_ts = np.zeros((len(DIM_Z),len(lam_list),len(gamma_list)))\n",
    "util_acc_va = np.zeros((len(DIM_Z),len(lam_list),len(gamma_list)))\n",
    "sens_acc_tr = np.zeros((len(DIM_Z),len(lam_list),len(gamma_list)))\n",
    "sens_acc_ts = np.zeros((len(DIM_Z),len(lam_list),len(gamma_list)))\n",
    "\n",
    "mi_u_ts = np.zeros((len(DIM_Z),len(lam_list),len(gamma_list)))\n",
    "mi_u_tr = np.zeros((len(DIM_Z),len(lam_list),len(gamma_list)))\n",
    "mi_s_ts = np.zeros((len(DIM_Z),len(lam_list),len(gamma_list)))\n",
    "mi_s_tr = np.zeros((len(DIM_Z),len(lam_list),len(gamma_list)))\n",
    "\n",
    "\n",
    "for i, dimz in enumerate(DIM_Z):\n",
    "    for j, lam in enumerate(lam_list):\n",
    "        for k, gamma in enumerate(gamma_list):\n",
    "            #Pre training\n",
    "            encoder, uncertainty_decoder, utility_decoder, autoencoder, z_discriminator, prior_generator, prior_gen_zdiscriminator, encoder_zdiscriminator, u_dircriminator, decoder_udiscriminator, s_dircriminator, decoder_sdiscriminator = get_full_model(dimz, lam, gamma)\n",
    "            pre_train_ae(dimz, lam, gamma, force_train=False, max_itr=100, batch_size=1024, verbose=2)\n",
    "            \n",
    "            print(\"Full model training\")\n",
    "            main_train(dimz, max_itr=50, batch_size=2048)\n",
    "\n",
    "            print(\"Evaluate performance of U\")\n",
    "            z_test = encoder.predict(x_test)\n",
    "            z_train = encoder.predict(x_train)\n",
    "            z_valid = encoder.predict(x_valid)\n",
    "            \n",
    "            u_test_hat = utility_decoder.predict(z_test)\n",
    "            u_train_hat = utility_decoder.predict(z_train)\n",
    "            u_valid_hat = utility_decoder.predict(z_valid)\n",
    "            if supervised:\n",
    "                util_acc_ts[i][j][k] = np.mean(np.argmax(u_test_hat,axis=1)==np.argmax(u_test,axis=1)) * 100\n",
    "                util_acc_tr[i][j][k] = np.mean(np.argmax(u_train_hat,axis=1)==np.argmax(u_train,axis=1)) * 100\n",
    "                util_acc_va[i][j][k] = np.mean(np.argmax(u_valid_hat,axis=1)==np.argmax(u_valid,axis=1)) * 100\n",
    "            else:\n",
    "                util_acc_ts[i][j][k] = np.mean(np.sum(np.square(u_test_hat-u_test), axis=(1,2,3)))\n",
    "                util_acc_tr[i][j][k] = np.mean(np.sum(np.square(u_train_hat-u_train), axis=(1,2,3)))\n",
    "                util_acc_va[i][j][k] = np.mean(np.sum(np.square(u_valid_hat-u_valid), axis=(1,2,3)))\n",
    "            \n",
    "            s_test_hat = uncertainty_decoder.predict(z_test)\n",
    "            sens_acc_ts[i][j][k] = np.mean(np.argmax(s_test_hat,axis=1)==np.argmax(s_test,axis=1)) * 100\n",
    "            s_train_hat = uncertainty_decoder.predict(z_train)\n",
    "            sens_acc_tr[i][j][k] = np.mean(np.argmax(s_train_hat,axis=1)==np.argmax(s_train,axis=1)) * 100\n",
    "            \n",
    "            if supervised:\n",
    "                print(\"Evaluate Mutual Information I(Z;U)\")            \n",
    "                mine = MINE(x_dim=dimz, y_dim=DIM_U)\n",
    "                _, mi_u_ts[i][j][k] = mine.fit(z_test, u_test, epochs=250, batch_size=1024, verbose=0)\n",
    "                _, mi_u_tr[i][j][k] = mine.fit(z_train, u_train, epochs=250, batch_size=1024, verbose=0)\n",
    "            \n",
    "            print(\"Evaluate Mutual Information I(Z;S)\")\n",
    "            mine = MINE(x_dim=dimz, y_dim=DIM_S)\n",
    "            _, mi_s_ts[i][j][k] = mine.fit(z_test, s_test, epochs=250, batch_size=1024, verbose=0)\n",
    "            _, mi_s_tr[i][j][k] = mine.fit(z_train, s_train, epochs=250, batch_size=1024, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if supervised:\n",
    "    _, u_test_hat = autoencoder.predict(x_test)\n",
    "    print(\"acc = \", np.mean(np.argmax(u_test_hat,axis=1)==np.argmax(u_test,axis=1)) * 100)\n",
    "else:\n",
    "    print( \"MSE = \", np.mean(np.square(u_test_hat-u_test)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results\n",
    "## Plot accuracy (or MSE in unsupervised mode)\n",
    "### Note: For all Gamma we have a picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "for i in range(util_acc_ts.shape[0]):\n",
    "    plt.figure(figsize=(15,8))\n",
    "#     if supervised:\n",
    "#         plt.title(r'Accuracy ' + f'{exp_info}_d={DIM_Z[i]}', fontsize=16)\n",
    "#     else:\n",
    "#         plt.title(r'Accuracy and MSE ' + f'{exp_info}_d={DIM_Z[i]}', fontsize=16)\n",
    "        \n",
    "    for j in range(util_acc_ts.shape[1]):\n",
    "        if supervised:\n",
    "            plt.plot(util_acc_ts[i][j], label=r'Utility Att Acc., Test, $\\lambda=$' + f\"{lam_list[j]}\", linestyle=linestyles[0], linewidth=2)\n",
    "            \n",
    "        plt.plot(sens_acc_ts[i][j], label=r'Sensitive Att Acc., Test, $\\lambda=$' + f\"{lam_list[j]}\", linestyle=linestyles[0], linewidth=2)\n",
    "        \n",
    "\n",
    "\n",
    "    plt.legend(prop={'size': 16})\n",
    "    plt.grid()\n",
    "    plt.xticks(list(range(len(gamma_list))), gamma_list, fontsize=16, rotation=90)\n",
    "    plt.xlabel(r'$\\gamma$', fontsize=24)\n",
    "    if supervised:\n",
    "        plt.ylabel(r'Accuracy on $\\mathbf{U}$ and $\\mathbf{S}$', fontsize=16)\n",
    "    else:\n",
    "        plt.ylabel(r'Accuracy on $\\mathbf{S}$', fontsize=16)\n",
    "    \n",
    "    plt.savefig(f'./saved_figures/chart_celeba_acc_d_{DIM_Z[i]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "    plt.savefig(f'./saved_figures/chart_celeba_acc_d_{DIM_Z[i]}_{exp_info}.png', format='png', bbox_inches='tight')\n",
    "\n",
    "        \n",
    "if supervised == False:\n",
    "    for i in range(util_acc_ts.shape[0]):\n",
    "        plt.figure(figsize=(15,8))\n",
    "#         plt.title(r'MSE ' + f'{exp_info}_d={DIM_Z[i]}', fontsize=16)\n",
    "        \n",
    "        for j in range(util_acc_ts.shape[1]):\n",
    "            plt.plot(util_acc_ts[i][j], label=r'Utility Att MSE., Test, $\\lambda=$' + f\"{lam_list[j]}\", linestyle=linestyles[0], linewidth=2)\n",
    "        \n",
    "        plt.legend(prop={'size': 16})\n",
    "        plt.grid()\n",
    "        plt.xticks(list(range(len(gamma_list))), gamma_list, fontsize=16, rotation=90)\n",
    "        plt.xlabel(r'$\\gamma$', fontsize=24)\n",
    "        plt.ylabel(r'MSE on $\\mathbf{U}$', fontsize=16)\n",
    "        plt.savefig(f'./saved_figures/chart_celeba_mse_d_{DIM_Z[i]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "        plt.savefig(f'./saved_figures/chart_celeba_mse_d_{DIM_Z[i]}_{exp_info}.png', format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracy (or MSE in unsupervised mode) (for each z and lambda we plot a chart for accuracy of the U and S)\n",
    "### Note: For each Gamma we have a picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "for i in range(util_acc_ts.shape[0]):\n",
    "    for j in range(util_acc_ts.shape[1]):\n",
    "        plt.figure(figsize=(15,8))\n",
    "        if supervised:\n",
    "#             plt.title(r'Accuracy ' + f'{exp_info}_d={DIM_Z[i]}_lam={lam_list[j]}', fontsize=16)\n",
    "            plt.plot(util_acc_ts[i][j], label=r'Utility Att Acc., Test', color=\"r\", linestyle=linestyles[0], linewidth=3)\n",
    "            plt.plot(util_acc_tr[i][j], label=r'Utility Att Acc., Train', color=\"g\", linestyle=linestyles[2], linewidth=3)\n",
    "            plt.plot(util_acc_va[i][j], label=r'Utility Att Acc., Validation', color=\"b\", linestyle=linestyles[3], linewidth=3)\n",
    "            \n",
    "            plt.plot(sens_acc_ts[i][j], label=r'Sensitive Att Acc., Test', linestyle=linestyles[0], linewidth=3)\n",
    "            plt.plot(sens_acc_tr[i][j], label=r'Sensitive Att Acc., Train', linestyle=linestyles[1], linewidth=2)\n",
    "        else:\n",
    "#             plt.title(r'Accuracy and MSE ' + f'{exp_info}_d={DIM_Z[i]}_lam={lam_list[j]}', fontsize=16)\n",
    "            plt.plot(util_acc_ts[i][j], label=r'Utility Att MSE., Test', color=\"r\", linestyle=linestyles[0], linewidth=3)\n",
    "            plt.plot(util_acc_tr[i][j], label=r'Utility Att MSE., Train', color=\"g\", linestyle=linestyles[1], linewidth=3)\n",
    "            plt.plot(util_acc_va[i][j], label=r'Utility Att MSE., Validation', color=\"b\", linestyle=linestyles[2], linewidth=3)\n",
    "\n",
    "        plt.legend(prop={'size': 16})\n",
    "        plt.grid()\n",
    "        plt.xticks(list(range(len(gamma_list))), gamma_list, fontsize=16, rotation=90)\n",
    "        plt.xlabel(r'$\\gamma$', fontsize=24)\n",
    "        if supervised:\n",
    "            plt.ylabel(r'Accuracy on $\\mathbf{U}$ and $\\mathbf{S}$', fontsize=16)\n",
    "            plt.savefig(f'./saved_figures/chart_celeba_acc_d_{DIM_Z[i]}_lambda_{lam_list[j]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "            plt.savefig(f'./saved_figures/chart_celeba_acc_d_{DIM_Z[i]}_lambda_{lam_list[j]}_{exp_info}.png', format='png', bbox_inches='tight')\n",
    "        else:\n",
    "            plt.ylabel(r'MSE on $\\mathbf{U}$', fontsize=16)\n",
    "            plt.savefig(f'./saved_figures/chart_celeba_mse_d_{DIM_Z[i]}_lambda_{lam_list[j]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "            plt.savefig(f'./saved_figures/chart_celeba_mse_d_{DIM_Z[i]}_lambda_{lam_list[j]}_{exp_info}.png', format='png', bbox_inches='tight')\n",
    "\n",
    "if not supervised:\n",
    "    for i in range(util_acc_ts.shape[0]):\n",
    "        for j in range(util_acc_ts.shape[1]):\n",
    "            plt.figure(figsize=(15,8))\n",
    "\n",
    "    #       plt.title(r'Accuracy ' + f'{exp_info}_d={DIM_Z[i]}_lam={lam_list[j]}', fontsize=16)\n",
    "\n",
    "            plt.plot(sens_acc_ts[i][j][:], label=r'Sensitive Att Acc., Test', linestyle=linestyles[0], linewidth=3)\n",
    "            plt.plot(sens_acc_tr[i][j][:], label=r'Sensitive Att Acc., Train', linestyle=linestyles[1], linewidth=3)\n",
    "\n",
    "            plt.legend(prop={'size': 16})\n",
    "            plt.grid()\n",
    "            plt.xticks(list(range(len(gamma_list))), gamma_list, fontsize=16, rotation=90)\n",
    "            plt.xlabel(r'$\\gamma$', fontsize=24)\n",
    "            plt.ylabel(r'Accuracy on $\\mathbf{S}$', fontsize=24)\n",
    "            plt.savefig(f'./saved_figures/chart_celeba_acc_d_{DIM_Z[i]}_lambda_{lam_list[j]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "            plt.savefig(f'./saved_figures/chart_celeba_acc_d_{DIM_Z[i]}_lambda_{lam_list[j]}_{exp_info}.png', format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot mutual information between Z and S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "colors = ['b', 'r', 'g', 'c']\n",
    "for i in range(mi_s_ts.shape[0]):\n",
    "    # plt.title(r'Mutual Information between $\\mathbf{S}$ and $\\mathbf{Z}$' + f' - dz_{DIM_Z[i]}_{exp_info}', fontsize=20)\n",
    "    plt.figure(figsize=(15,8))\n",
    "    for j in range(mi_s_ts.shape[1]):\n",
    "        plt.plot(mi_s_ts[i][j], label=r'Test, $d_z=$' + f'{DIM_Z[i]}' + r', $\\lambda=$' + f\"{lam_list[j]}\", linestyle=linestyles[0], c=colors[j%len(colors)] ,linewidth=3)\n",
    "        plt.plot(mi_s_tr[i][j], label=r'Train, $d_z=$' + f'{DIM_Z[i]}' + r', $\\lambda=$' + f\"{lam_list[j]}\", linestyle=linestyles[1], c=colors[j%len(colors)] ,linewidth=3)\n",
    "        \n",
    "    # plt.xscale('linear')\n",
    "    plt.xlabel(r'$\\gamma$', fontsize=24)\n",
    "    plt.xticks(list(range(len(gamma_list))), gamma_list, rotation=90)\n",
    "\n",
    "    plt.ylabel(r'I($\\mathbf{S}$;$\\mathbf{Z}$)', fontsize=24)\n",
    "    plt.legend(prop={'size': 16})\n",
    "    plt.grid()\n",
    "    plt.savefig(f'./saved_figures/chart_celeba_mi_zs_d_{DIM_Z[i]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "    plt.savefig(f'./saved_figures/chart_celeba_mi_zs_d_{DIM_Z[i]}_{exp_info}.png', format='png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot mutual information between Z and U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if supervised:\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    for i in range(mi_u_ts.shape[0]):\n",
    "        plt.figure(figsize=(15,8))\n",
    "#         plt.title(r'Mutual Information between $\\mathbf{U}$ and $\\mathbf{Z}$' + f' - dz_{DIM_Z[i]}_{exp_info}', fontsize=20)\n",
    "        linestyles = ['-', '--', '-.', ':']\n",
    "        colors = ['b', 'r', 'g', 'c']\n",
    "        for j in range(mi_u_ts.shape[1]):\n",
    "            plt.plot(mi_u_ts[i][j], label=r'Test, $d_z=$' + f'{DIM_Z[i]}' + r', $\\lambda=$' + f\"{lam_list[j]}\", linestyle=linestyles[0], c=colors[j%len(colors)] ,linewidth=3)\n",
    "            plt.plot(mi_u_tr[i][j], label=r'Train, $d_z=$' + f'{DIM_Z[i]}' + r', $\\lambda=$' + f\"{lam_list[j]}\", linestyle=linestyles[1], c=colors[j%len(colors)] ,linewidth=3)\n",
    "\n",
    "        # plt.xscale('linear')\n",
    "        plt.xlabel(r'$\\gamma$', fontsize=24)\n",
    "        plt.xticks(list(range(len(gamma_list))), gamma_list,rotation=90)\n",
    "\n",
    "        plt.ylabel(r'I($\\mathbf{U}$;$\\mathbf{Z}$)', fontsize=24)\n",
    "        plt.legend(prop={'size': 16})\n",
    "        plt.grid()\n",
    "        plt.savefig(f'./saved_figures/chart_celeba_mi_zu_d_{DIM_Z[i]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "        plt.savefig(f'./saved_figures/chart_celeba_mi_zu_d_{DIM_Z[i]}_{exp_info}.png', format='png', bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results values to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat(f'./saved_data/acc_celeba_{exp_info}.mat', {'util_acc_ts':util_acc_ts, 'util_acc_tr':util_acc_tr, 'util_acc_va':util_acc_va, 'sens_acc_ts':sens_acc_ts, 'sens_acc_tr':sens_acc_tr})\n",
    "sio.savemat(f'./saved_data/mi_celeba_{exp_info}.mat', {'mi_s_ts':mi_s_ts, 'mi_s_tr':mi_s_tr, 'mi_u_ts':mi_u_ts, 'mi_u_tr':mi_u_tr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load results from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "mat_contents  = sio.loadmat(f'./saved_data/acc_celeba_{exp_info}.mat')\n",
    "util_acc_ts = mat_contents['util_acc_ts']\n",
    "util_acc_tr = mat_contents['util_acc_tr']\n",
    "util_acc_va = mat_contents['util_acc_va']\n",
    "sens_acc_ts = mat_contents['sens_acc_ts']\n",
    "sens_acc_tr = mat_contents['sens_acc_tr']\n",
    "\n",
    "\n",
    "mat_contents  = sio.loadmat(f'./saved_data/mi_celeba_{exp_info}.mat')\n",
    "mi_s_tr = mat_contents['mi_s_tr']\n",
    "mi_s_ts = mat_contents['mi_s_ts']\n",
    "mi_u_ts = mat_contents['mi_u_ts']\n",
    "mi_u_tr = mat_contents['mi_u_tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_list = [0.0001, 0.1, 10, 1000]\n",
    "gamma_list = [0.0001, 0.1, 10, 1000]\n",
    "DIM_Z = [64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf24",
   "language": "python",
   "name": "tf24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
